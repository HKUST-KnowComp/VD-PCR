MB-pseudo {
  mode = train
  use_coref = true
  train_on_dense = false
  metrics_to_maximize = prp_f1

  # visdial
  visdial_image_feats = data/visdial/visdial_img_feat.lmdb

  visdial_train = data/processed/visdial_1.0_train_with_vispro_bert_tokenized.json
  visdial_val = data/processed/visdial_1.0_val_with_vispro_only_bert_tokenized.json
  visdial_test = data/processed/visdial_1.0_test_with_vispro_only_bert_tokenized.json
  visdial_val_dense_annotations = data/processed/visdial_1.0_val_dense_annotations.json

  start_path = checkpoints-release/basemodel
  model_config = config/bert_base_6layer_6conect.json
  dataloader_text_only = false
  rlv_hst_only = false
  rlv_hst_dense_round = false

  # visdial training
  visdial_tot_rounds = 11
  mask_prob = 0
  image_mask_prob = 0.1
  num_negative_samples = 0
  sequences_per_image = 2
  max_seq_len = 256
  num_options = 100
  num_options_dense = 100
  model_config = config/bert_base_6layer_6conect.json
  use_embedding = vilbert
  skip_mrr_eval = false
  skip_eval = false
  batch_size = 4
  eval_batch_size = 4
  lm_loss_coeff = 0
  nsp_loss_coeff = 0
  img_loss_coeff = 0
  visdial_loss_coeff = 0
  coref_loss_coeff = 1
  batch_multiply = 1
  use_trainval = false
  dense_loss = ce
  continue_evaluation = false
  coref_only = false
  eval_at_start = false

  # coref computation limits
  max_sent_num = 11
  max_top_antecedents = 50
  top_span_ratio = 0.4

  # coref model config
  initializer = normal
  bert_cased = false
  feature_size = 20
  span_embedding_dim = 768
  max_span_width = 10
  ffnn_size = 1000
  ffnn_depth = 1
  coref_depth = 2
  dropout_prob = 0.3
  fast_ant_score_dropout_prob = 0.5
  span_width_scorer_depth = 1
  mention_scorer_depth = 1
  slow_ant_scorer_depth = 1

  # restore ckpt
  loads_best_ckpt = false
  loads_ckpt = false
  restarts = false
  resets_max_metric = false
  uses_new_optimizer = false
  sets_new_lr = false
  loads_start_path = false

  # training
  random_seed = 2020
  next_logging_pct = 5
  next_evaluating_pct = 50.0
  max_ckpt_to_keep = 1
  num_epochs = 10
  early_stop_epoch = 5
  skip_saving_ckpt = false
  use_apex = false
  dp_type = ddp
  stop_epochs = -1
  train_each_round = false
  drop_last_answer = false

  # predicting
  predict_split = test
  predict_shards_num = 0
  predict_shard = 0
  predict_each_round = false
  predict_dense_round = false
  num_test_dialogs = 8000
  num_val_dialogs = 2064
  save_score = false

  # optimizer
  reset_optim = none
  learning_rate_bert = 1e-5
  learning_rate_task = 5e-5
  min_lr = 1e-6
  decay_method_bert = linear
  decay_method_task = linear
  decay_exp = 2
  max_grad_norm = 0
  task_optimizer = adam
  warmup_ratio = 0.1

  # directory
  log_dir = logs/conly
  data_dir = data
  visdial_output_dir = visdial_output
  coref_output_dir = coref_output
  bert_cache_dir = transformers

  # data
  id_to_genre = ["dl", "bn", "mz", "nw", "pt", "tc", "wb"]
}

MB-J_eval = ${MB-pseudo} {
  use_embedding = joint
  loads_start_path = true
  start_path = logs/joint/MB-J/epoch_best.ckpt
}

MB-JC_eval = ${MB-J_eval} {
  model_config = config/bert_base_6layer_6conect_coref_heads.json
  use_embedding = joint_head
  start_path = logs/joint/MB-JC/epoch_best.ckpt
  span_embedding_dim = 320
  ffnn_size = 512
}
